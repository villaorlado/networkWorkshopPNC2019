{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence network null models\n",
    "with Andy and Miguel\n",
    "\n",
    "___\n",
    "\n",
    "In this notebook, we'll explore the use of network null models to identify special structural features of co-occurrence networks, using methods we applied to [Javanese _wayang kulit_](http://jhnr.uni.lu/index.php/jhnr/article/view/42).\n",
    "___\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First, choose a data set you'd like to work with by defining the variable \"dataset\" in the cell below as one of the following strings:\n",
    "\n",
    "* 'WayangAdegan' (Javanese _wayang kulit Mahabharata_ , finer scene-level co-occurrence window)\n",
    "* 'WayangLakon' (Javanese _wayang kulit Mahabharata_ , coarser story-level co-occurrence window)\n",
    "* 'Mahabharat1988' (B.R. Chopra's 1988 serial TV adaption of the _Mahabharata_ )\n",
    "* 'Mahabharat2013' (2013 serial TV adaption of the _Mahabharata_ )\n",
    "* 'AntonyAndCleopatra' (Shakespeare)\n",
    "* 'RomeoAndJuliet' (Shakespeare)\n",
    "* 'InfinityWar' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'InfinityGauntlet' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'InfinityCrusade' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'Annihilation' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'CivilWar' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'InfinityCrusade' (Marvel Comics story arc from comicbookdb.com)\n",
    "* 'DarkPhoenixSaga' (Marvel Comics story arc from comicbookdb.com)\n",
    "\n",
    "Alternatively, prepare your own character lists in the same format as the other data files in the folder \"./data/\", save it there with of the form \"YOURTITLE_CharactersByEpisode.txt\", and define \"dataset = 'YOURTITLE'\" in the cell below.\n",
    "\n",
    "Then run the cells below to load the modules and functions that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AntonyAndCleopatra'\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import errno\n",
    "try:\n",
    "    os.makedirs(\"./gephi/\")\n",
    "except OSError as exception:\n",
    "    if exception.errno != errno.EEXIST:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadNetworks(dataset):\n",
    "    \n",
    "    EpisodeNodes, CharacterNodes, B = ConstructBipartite(dataset)\n",
    "    G = CharacterCooccurrenceNetwork(B, EpisodeNodes, CharacterNodes)\n",
    "    GI = InverseWeightNetwork(G)\n",
    "    E = EpisodeIntersectionNetwork(B, EpisodeNodes, CharacterNodes)\n",
    "    EI = InverseWeightNetwork(E)\n",
    "\n",
    "    return EpisodeNodes, CharacterNodes, B, G, GI, E, EI\n",
    "\n",
    "\n",
    "def ConstructBipartite(dataset):\n",
    "\n",
    "    episodelist = open(\n",
    "        \"./data/\" + dataset + \"_CharactersByEpisode.txt\").read().splitlines()\n",
    "\n",
    "    characters = []\n",
    "    episodes = dict()\n",
    "    B = nx.Graph()\n",
    "    for episode in episodelist:\n",
    "        episodes[episode.split('=')[0].strip(' ')] = [\n",
    "        char.strip(\" \") for char in episode.split('=')[1].split(',')\n",
    "        if char.strip(\" \")]\n",
    "        characters.extend([\n",
    "        char.strip(\" \") for char in episode.split('=')[1].split(',')\n",
    "        if char.strip(\" \")])\n",
    "    EpisodeNodes = sorted(set(episodes.keys()))\n",
    "    CharacterNodes = sorted(\n",
    "        set([character.strip(\" \") for character in characters]))\n",
    "\n",
    "    B.add_nodes_from(EpisodeNodes)\n",
    "    B.add_nodes_from(CharacterNodes)\n",
    "\n",
    "    for ep in EpisodeNodes:\n",
    "        for char in episodes[ep]:\n",
    "            B.add_edge(ep,char)\n",
    "\n",
    "    return EpisodeNodes, CharacterNodes, B\n",
    "\n",
    "def CharacterCooccurrenceNetwork(B, EpisodeNodes, CharacterNodes):\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(CharacterNodes)\n",
    "    for c1 in CharacterNodes:\n",
    "        for c2 in CharacterNodes:\n",
    "            for L in EpisodeNodes:\n",
    "                if (B.has_edge(L,c1)) and (B.has_edge(L,c2)) and (c1 is not c2):\n",
    "                    if G.has_edge(c1,c2):\n",
    "                        G[c1][c2]['weight'] += 1./2\n",
    "                    else:\n",
    "                        G.add_edge(c1,c2, weight=1./2)\n",
    "\n",
    "    return G\n",
    "\n",
    "def InverseWeightNetwork(G):\n",
    "\n",
    "    GI=nx.Graph()\n",
    "    GI.add_nodes_from(G.nodes())\n",
    "    for edge in G.edges():\n",
    "        GI.add_edge(edge[0],edge[1],weight = \\\n",
    "            1./float(G[edge[0]][edge[1]]['weight']))\n",
    "\n",
    "    return GI\n",
    "\n",
    "def EpisodeIntersectionNetwork(B, EpisodeNodes, CharacterNodes):\n",
    "\n",
    "    E = nx.Graph()\n",
    "    E.add_nodes_from(EpisodeNodes)\n",
    "    for e1 in EpisodeNodes:\n",
    "        counter = 0\n",
    "        for e2 in EpisodeNodes:\n",
    "            for C in CharacterNodes:\n",
    "                if (B.has_edge(C,e1)) and (B.has_edge(C,e2)) and (e1 is not e2):\n",
    "                    counter += 1\n",
    "                    if E.has_edge(e1,e2):\n",
    "                        E[e1][e2]['weight'] += 1./2 # because of double-counting\n",
    "                    else:\n",
    "                        E.add_edge(e1,e2, weight=1./2)\n",
    "\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "___\n",
    "\n",
    "## Part 1: Reviewing \"raw\" centrality measures\n",
    "\n",
    "As discussed before, the tools of network analysis provide a means of answering questions like,\n",
    "_\"Which characters play unique structural roles in the network?\"_ Our initial motivation in applying network analysis to the Javanese _wayang kulit_ retelling of the _Mahabharata_ was specifically to seek out otherwise-\"hidden\" patterns within the epic's scaffolding of character encounters that traditional approaches would tend to overlook. However, some aspects of a character's \"importance\" within a story (such as their overall number of appearances) could be considered, and even quantified, without any need to resort to a network perspective. Our desire to disentangle (insofar far as is possible) a character's structural role from its overall number of appearances motivated our adoption of network null models.\n",
    "\n",
    "Let's review some network centrality measures on your chosen _character co-occurrence network_ again, and examine how they relate to characters' numbers of appearances in episodes ( i.e. their _degrees_ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EpisodeNodes, CharacterNodes, B, G, GI, E, EI = LoadNetworks(dataset)\n",
    "\n",
    "CharacterDegree = dict(B.degree(CharacterNodes))\n",
    "CharacterNodeStrength = nx.degree( G , weight = 'weight' )\n",
    "CharacterTBetweenness = nx.betweenness_centrality( G )\n",
    "CharacterBetweenness = nx.betweenness_centrality( GI , weight = 'weight' )\n",
    "CharacterCloseness = nx.closeness_centrality( G )\n",
    "CharacterEig = nx.eigenvector_centrality( G , weight = 'weight' )\n",
    "\n",
    "cdeg = list(CharacterDegree.values())\n",
    "cstr = [d[1] for d in CharacterNodeStrength]\n",
    "ctbt = list(CharacterTBetweenness.values())\n",
    "cbet = list(CharacterBetweenness.values())\n",
    "cclo = list(CharacterCloseness.values())\n",
    "ceig = list(CharacterEig.values())\n",
    "\n",
    "plt.plot(cdeg,cstr,'k.')\n",
    "plt.xlabel('Node degree')\n",
    "plt.ylabel('Node strength', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cdeg,cbet,'k.')\n",
    "plt.xlabel('Node degree')\n",
    "plt.ylabel('Betweenness centrality', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cdeg,ctbt,'k.')\n",
    "plt.xlabel('Node degree')\n",
    "plt.ylabel('Betweenness centrality (unweighted)', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cdeg,cclo,'k.')\n",
    "plt.xlabel('Node degree')\n",
    "plt.ylabel('Closeness centrality', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cdeg,ceig,'k.')\n",
    "plt.xlabel('Node degree')\n",
    "plt.ylabel('Eigenvector centrality', fontsize=14)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions to ponder and discuss:\n",
    "\n",
    "* What trends do you see? How do each of these centrality measures vary with respect to a character's number of appearances?\n",
    "\n",
    "* Recalling how these centrality measures are defined, can you explain why you see these general trends? How might our use of weighted networks (instead of networks with unweighted, binary links) affect these trends?\n",
    "\n",
    "* Our initial goal was to seek out otherwise-_hidden_ structural patterns that we probably wouldn't have noticed without the help of network science. How might the trends observed above this limit our ability to identify overlooked _\"characters with unique structural roles\"_ using network centrality measures?\n",
    "\n",
    "* How might we get around this obstacle?\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Part 2: Identifying centrality outliers using null models\n",
    "\n",
    "### Refining our question...\n",
    "\n",
    "Our aim in using network analysis as an exploratory tool was to reveal _otherwise-hidden_ patterns that we probably wouldn't have noticed just by talking about, say, which characters appear most often. Ideally, this approach should bring to the forefront lower-profile, less-familiar characters with interesting roles in the network, as opposed to just being a very roundabout way to recreate lists of the most frequently-appearing characters.\n",
    "\n",
    "Instead of asking, _\"which nodes are important in the network?\"_ , maybe we should have been more specific: \n",
    "\"Which characters have unique structural roles **that can't merely be attributed to how often they appear**?\". Let's look for characters who have centrality values that we wouldn't **expect** them to have based on their degrees.\n",
    "\n",
    "\n",
    "## Null models\n",
    "\n",
    "In order to do this -- to look for characters having centrality values that we wouldn't **expect** them to have based on their degrees -- we first have to know what we expect. That is, _\"What centrality values do we expect a character to have within a co-occurrence network based on its number of appearances?\"_ .\n",
    "\n",
    "We answer this question by generating a bunch of artificial co-occurrence networks in which characters still have the same numbers of appearances, but in which other details are free to vary. Imagine 1000 alternate, parallel universe versions of the story. In each of these parallel universe versions, the frequently-appearing characters appear just as frequently, and one-off characters still appear only once. If some scenes in the real story depict many characters, while others feature only a few, the same will also be true of the story's parallel universe counterparts. However, details of each scene, such as the specific combinations of characters that appear together in each of these scenes, may be completely different. If some feature of the empirical network is shared by all these parallel-universe versions, then their shared features might explain this similarity. However, if the empirical network has some special feature that none of its 1000 parallel-universe counterparts share, then we now have reason to argue that this feature can't be explained just by the thing they have in common (their _degree sequences_). If the empirical network has some special feature that 10 out of its 1000 parallel-universe counterparts share, then we can still perhaps make that argument... just not quite as strongly.\n",
    "\n",
    "For example, if a character has higher betweenness centrality in the empirical network than it does in 99.9% of null model networks that share the same degree sequence, then we have reason to claim that this character's position \"in between\" other characters can't just be attributed to its number of appearances, or the degree sequence in general. We can can reject the _null hypothesis_ that its betweenness is explained by its number of appearances, and confidently assert that there must be some more special, not-so-random patterns of co-occurrence going on that place the character in that special position.\n",
    "\n",
    "So, our goal is to generate a bunch of new networks that share the degree sequence of the empirical network's _degree sequences_ (that is, each character's number of scenes) and underlying _bipartite_ structure. First, let's do it using a _configuration model_.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "### Configuration model\n",
    "\n",
    "In a _configuration model_, we take the _degree sequences_ (that is, the list of characters and their numbers of appearances, as well as the list of episodes and their numbers of characters) from our empirical network of character-episode affiliations, and attempt to randomly generate a network that has (approximately) the same degree sequence. We do so buy considering each possible pairing of a character $c$ (with degree $d(c)$ in the empirical network) and episode $e$ (with degree $d(e)$ in the empirical network), and adding a link between them with probability\n",
    "\n",
    "$$ P \\left( c \\mathrm{ \\ is \\ linked \\ to \\ } e \\right) =  \\frac{d(c) \\times d(e)}{\\mathrm{Total \\ number \\ of \\ links}} $$\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration model code\n",
    "\n",
    "The function below generates a network realization according to the configuration model above given the empirical network's degree sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BipartiteConfigurationModelGenerate(EpisodeDegrees, CharacterDegrees):\n",
    "\n",
    "    EpisodeNodes = list(EpisodeDegrees.keys())\n",
    "    CharacterNodes = list(CharacterDegrees.keys())\n",
    "\n",
    "    C = nx.Graph()\n",
    "    C.add_nodes_from(CharacterDegrees.keys())\n",
    "    C.add_nodes_from(EpisodeDegrees.keys())\n",
    "\n",
    "    NumberOfLinks = sum(CharacterDegrees.values())\n",
    "\n",
    "    for character in CharacterNodes:\n",
    "        for episode in EpisodeNodes: \n",
    "            if ( np.random.random() < np.divide(np.multiply(float(CharacterDegrees[character]),\n",
    "                    float(EpisodeDegrees[episode])), float(NumberOfLinks))):\n",
    "                C.add_edge(character, episode)\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### More questions to ponder:\n",
    "\n",
    "1) If a null model network is constructed by generating links randomly in accord with the above probability, will its degree sequence exactly match that of the empirical network?\n",
    "\n",
    "2) What issues might this _configuration model_ approach introduce in the analysis of low-degree characters?\n",
    "\n",
    "3) Imagine that you wanted to better understand the role that sizes of episodes played in the features of the co-occurrence network. For example, imagine that in a certain traditional genre of storytelling, it is customary for many scenes to feature only a small number of characters, while a few select scenes, many characters appear at once. Suppose you want to address the research question, _To what extent does this convention affect/explain the features of the resulting co-occurrence network?_ How could you alter the above configuration null model to investigate this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Generating configuration model networks\n",
    "\n",
    "Let's now generate an ensemble of _configuration model_ networks to use as a null model for comparison to our story's co-occurrence network. Each time we generate a bipartite network of character-episode affiliations, we will then create the corresponding character co-occurrence network, and then calculate some node and networks metrics on this network and record the results.\n",
    "\n",
    "After we generate the ensemble by running the cell below, we will then go on to look at some of these results. A \"sufficiently large\" ensemble of networks is necessary for us to get a good approximation for the probability distributions of each of our network metrics. If later we find that our initial ensemble was too small to produce smooth distributions, we may wish to alter the value of the variable 'EnsembleSize' and then re-run the cell below to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleSize = 50\n",
    "Links = False\n",
    "\n",
    "\n",
    "EpisodeDegree = dict(B.degree(EpisodeNodes))\n",
    "\n",
    "EnsembleBC = {}\n",
    "RunningMeanBC = {}\n",
    "EnsembleCC = {}\n",
    "RunningMeanCC = {}\n",
    "EnsembleEC = {}\n",
    "RunningMeanEC = {}\n",
    "EnsembleDegree = {}\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    EnsembleBC[character] = []\n",
    "    RunningMeanBC[character] = []\n",
    "    EnsembleCC[character] = []\n",
    "    RunningMeanCC[character] = []\n",
    "    EnsembleEC[character] = []\n",
    "    RunningMeanEC[character] = []\n",
    "    EnsembleDegree[character] = []\n",
    "    \n",
    "EnsembleDiameter = []\n",
    "EnsembleShortestPath = []\n",
    "EnsembleDensity = []\n",
    "EnsembleClustering = []\n",
    "\n",
    "if Links:\n",
    "    EnsembleLinkWeight = {}\n",
    "    EnsembleLinkBC = {}\n",
    "    for edge in G.edges():\n",
    "        EnsembleLinkWeight[tuple(sorted(edge))] = []\n",
    "        EnsembleLinkBC[tuple(sorted(edge))] = []\n",
    "        \n",
    "for t in range(EnsembleSize):\n",
    "    if ((t%10)==0):\n",
    "        print(\"Generating null model network realization \",t+1)\n",
    "    C = BipartiteConfigurationModelGenerate(EpisodeDegree, CharacterDegree)\n",
    "    H = CharacterCooccurrenceNetwork(C, EpisodeNodes, CharacterNodes)\n",
    "    HI = InverseWeightNetwork(H)\n",
    "    CBetweenness = nx.betweenness_centrality(HI, weight='weight')\n",
    "    CCloseness = nx.closeness_centrality(H)\n",
    "    CEigenvector = nx.eigenvector_centrality(H, weight='weight')\n",
    "    CDegree = nx.degree(C)        \n",
    "    for character in CharacterNodes:\n",
    "        EnsembleBC[character].append(CBetweenness[character])\n",
    "        RunningMeanBC[character].append(np.mean(EnsembleBC[character]))\n",
    "        EnsembleCC[character].append(CCloseness[character])\n",
    "        RunningMeanCC[character].append(np.mean(EnsembleCC[character]))\n",
    "        EnsembleEC[character].append(CEigenvector[character])\n",
    "        RunningMeanEC[character].append(np.mean(EnsembleEC[character]))\n",
    "        EnsembleDegree[character].append(CDegree[character])\n",
    "    if (nx.number_connected_components(H)>1):\n",
    "        Hcomps = list(nx.connected_component_subgraphs(H))\n",
    "        HLC = Hcomps[[len(h.nodes()) for h in Hcomps].index(np.max([len(h.nodes()) for h in Hcomps]))]\n",
    "        HLCI = InverseWeightNetwork(HLC)\n",
    "        EnsembleDiameter.append(nx.diameter(HLC))\n",
    "        EnsembleShortestPath.append(nx.average_shortest_path_length(HLCI, weight=\"weight\"))\n",
    "        EnsembleDensity.append(nx.density(HLC))\n",
    "        EnsembleClustering.append(nx.average_clustering(HLC,weight=\"weight\"))\n",
    "    else:\n",
    "        EnsembleDiameter.append(nx.diameter(HI))\n",
    "        EnsembleShortestPath.append(nx.average_shortest_path_length(HI, weight=\"weight\"))\n",
    "        EnsembleDensity.append(nx.density(H))\n",
    "        EnsembleClustering.append(nx.average_clustering(H,weight=\"weight\"))\n",
    "    if Links:\n",
    "        CLinkBetweenness = nx.edge_betweenness_centrality(HI, weight=\"weight\", normalized=True)\n",
    "        for edge in CLinkBetweenness.keys():\n",
    "            if tuple(sorted(edge)) in EnsembleLinkWeight.keys():\n",
    "                EnsembleLinkWeight[tuple(sorted(edge))].append(H[edge[0]][edge[1]][\"weight\"])\n",
    "                EnsembleLinkBC[tuple(sorted(edge))].append(CLinkBetweenness[edge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining null model results\n",
    "\n",
    "#### Betweenness centrality\n",
    "\n",
    "Having generated the ensemble of networks, let's see some of its results. \n",
    "\n",
    "The cell just below will print a list of characters sorted by their betweenness centrality in descending order for reference.  In the subsequent cell, we can choose a character name from this list by defining the variable \"character\" as desired (for example, \"character = 'Romeo'\".). Running the cell will then display the following:\n",
    "\n",
    "* The values of that character's betweenness centrality in the sequence of null model networks, compared to its empirical network value\n",
    "* The running mean of the character's null model betweenness centrality values in this sequence. We expect this curve to smooth out and approach to fairly constant value; if it hasn't yet, we may wish to go back a step, increase the value of 'EnsembleSize', and re-run the cell above to generate a larger ensemble.\n",
    "* A histogram of the character's null model betweenness centrality, with the empirical value and the corresponding \"$p$-value\". This \"$p$-value\" approximates the fraction of null model networks in which the measure at hand would exceed the actual value observed in the empirical network. That is to say, it represents the probability that this value could be achieved by random chance in co-occurrence networks with similar degree sequences.\n",
    "\n",
    "The default is set so that results will be shown for the highest-centrality character. Feel free to explore by re-running the cells below for different choices of the string variable \"character\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ (index+1,character) for index,character in enumerate(sorted(CharacterNodes, key=lambda x: CharacterBetweenness[x], reverse=True)) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = sorted(CharacterNodes, key=lambda x: CharacterBetweenness[x], reverse=True)[0]\n",
    "\n",
    "actualbc = CharacterBetweenness[character]\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualbc*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),EnsembleBC[character],'.')\n",
    "plt.xlim((0,EnsembleSize+1))\n",
    "plt.xlabel('Network realization')\n",
    "plt.ylabel('Betweenness centrality')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualbc*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),RunningMeanBC[character])\n",
    "plt.xlim((1,EnsembleSize+1))\n",
    "plt.xlabel('Number of realizations')\n",
    "plt.ylabel('Ensemble mean B.C.')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "    \n",
    "n, bins, __ = plt.hist(EnsembleBC[character])\n",
    "\n",
    "NumberOfBins = 1000\n",
    "\n",
    "binsc = .5*( bins[1:] + bins[:-1] )\n",
    "bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "diff = [np.abs(x-actualbc) for x in bins2c]\n",
    "ind = diff.index(np.min(diff))\n",
    "pval = np.sum(n2[ind:])/np.sum(n2)\n",
    "\n",
    "plt.plot([actualbc,actualbc],[0,np.max(n)],'r--')\n",
    "plt.plot(bins2c,n2,'k')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.xlabel('Betweenness centrality')\n",
    "plt.ylabel('Null model abundance')\n",
    "plt.title('Node: ' + character , fontsize=16 )\n",
    "plt.show()\n",
    "\n",
    "CPD = np.cumsum(n2)/np.sum(n2)\n",
    "plt.plot(bins2c, CPD,'k')\n",
    "plt.plot([actualbc,actualbc],[0,CPD[ind]],'r--')\n",
    "plt.plot([0,bins2c[ind]],[CPD[ind],CPD[ind]],'r--')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel('Betweenness centrality')\n",
    "plt.ylabel('Cumulative probability distribution')\n",
    "plt.title('Node: ' + character + \" ; $p$-value: 1 - \" + str(round(1-pval,4)) + \" = \" + str(round(pval,4)), fontsize=16 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness centrality\n",
    "\n",
    "Now, let's do the same for closeness centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ (index+1,character) for index,character in enumerate(sorted(CharacterNodes, key=lambda x: CharacterCloseness[x], reverse=True)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = sorted(CharacterNodes, key=lambda x: CharacterCloseness[x], reverse=True)[0]\n",
    "\n",
    "actualcc = CharacterCloseness[character]\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualcc*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),EnsembleCC[character],'.')\n",
    "plt.xlim((0,EnsembleSize+1))\n",
    "plt.xlabel('Network realization')\n",
    "plt.ylabel('Closeness centrality')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualcc*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),RunningMeanCC[character])\n",
    "plt.xlim((1,EnsembleSize+1))\n",
    "plt.xlabel('Number of realizations')\n",
    "plt.ylabel('Ensemble mean C.C.')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "    \n",
    "n, bins, __ = plt.hist(EnsembleCC[character])\n",
    "\n",
    "NumberOfBins = 1000\n",
    "\n",
    "binsc = .5*( bins[1:] + bins[:-1] )\n",
    "bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "diff = [np.abs(x-actualcc) for x in bins2c]\n",
    "ind = diff.index(np.min(diff))\n",
    "pval = np.sum(n2[ind:])/np.sum(n2)\n",
    "\n",
    "plt.plot([actualcc,actualcc],[0,np.max(n)],'r--')\n",
    "plt.plot(bins2c,n2,'k')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Null model abundance')\n",
    "plt.title('Node: ' + character , fontsize=16 )\n",
    "plt.show()\n",
    "\n",
    "CPD = np.cumsum(n2)/np.sum(n2)\n",
    "plt.plot(bins2c, CPD,'k')\n",
    "plt.plot([actualcc,actualcc],[0,CPD[ind]],'r--')\n",
    "plt.plot([0,bins2c[ind]],[CPD[ind],CPD[ind]],'r--')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Cumulative probability distribution')\n",
    "plt.title('Node: ' + character + \" ; $p$-value: 1 - \" + str(round(1-pval,4)) + \" = \" + str(round(pval,4)), fontsize=16 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector centrality\n",
    "\n",
    "Feel free to try the same for eigenvector centrality below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ (index+1,character) for index,character in enumerate(sorted(CharacterNodes, key=lambda x: CharacterEig[x], reverse=True)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = sorted(CharacterNodes, key=lambda x: CharacterEig[x], reverse=True)[0]\n",
    "\n",
    "actualec = CharacterEig[character]\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualec*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),EnsembleEC[character],'.')\n",
    "plt.xlim((0,EnsembleSize+1))\n",
    "plt.xlabel('Network realization')\n",
    "plt.ylabel('Eigenvector centrality')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,EnsembleSize+1),actualec*np.ones(EnsembleSize),'r--')\n",
    "plt.plot(range(1,EnsembleSize+1),RunningMeanEC[character])\n",
    "plt.xlim((1,EnsembleSize+1))\n",
    "plt.xlabel('Number of realizations')\n",
    "plt.ylabel('Ensemble mean E.C.')\n",
    "plt.title('Node: ' + character )\n",
    "plt.show()\n",
    "    \n",
    "n, bins, __ = plt.hist(EnsembleEC[character])\n",
    "\n",
    "NumberOfBins = 1000\n",
    "\n",
    "binsc = .5*( bins[1:] + bins[:-1] )\n",
    "bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "diff = [np.abs(x-actualec) for x in bins2c]\n",
    "ind = diff.index(np.min(diff))\n",
    "pval = np.sum(n2[ind:])/np.sum(n2)\n",
    "\n",
    "plt.plot([actualec,actualec],[0,np.max(n)],'r--')\n",
    "plt.plot(bins2c,n2,'k')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.xlabel('Eigenvector centrality')\n",
    "plt.ylabel('Null model abundance')\n",
    "plt.title('Node: ' + character , fontsize=16 )\n",
    "plt.show()\n",
    "\n",
    "CPD = np.cumsum(n2)/np.sum(n2)\n",
    "plt.plot(bins2c, CPD,'k')\n",
    "plt.plot([actualec,actualec],[0,CPD[ind]],'r--')\n",
    "plt.plot([0,bins2c[ind]],[CPD[ind],CPD[ind]],'r--')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel('Eigenvector centrality')\n",
    "plt.ylabel('Cumulative probability distribution')\n",
    "plt.title('Node: ' + character + \" ; $p$-value: 1 - \" + str(round(1-pval,4)) + \" = \" + str(round(pval,4)), fontsize=16 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Centrality outliers in context\n",
    "\n",
    "We have seen how the null model works to quantify how much an individual node's centrality values deviate from its expected values within the context of the given network's degree sequence. Having focused on individual nodes, now let's step back and summarize the results in context of the whole network.\n",
    "\n",
    "#### Ranking by $p$-values vs. raw centrality\n",
    "\n",
    "First, we will check whether or not we actually learn anything new from ranking nodes by $p$-value (from low to high) rather than by the raw values of a centrality measure. The code below computes $p$-values for all characters, and plots their adjusted ranks (#1 means lowest $p$-value) versus their original ranks (#1 means highest centrality). It then lists all characters in order of increasing $p$-value.\n",
    "\n",
    "To explore other node measures, change the assignment of the variable 'Pval' to the dictionary 'PvalB' (betweenness), 'PvalC' (closeness), or 'PvalE' (eigenvector).\n",
    "\n",
    "Also feel free to make changes to the list variable 'ShowNames' below if you wish to label some of the nodes (for example, \"ShowNames = [ 'Romeo' , 'Juliet' ]\" will label those characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NullModelPvalues(RawCentrality,EnsembleCentrality):\n",
    "\n",
    "    Pval = {}\n",
    "\n",
    "    for character in CharacterNodes:\n",
    "\n",
    "        actualc = RawCentrality[character]\n",
    "\n",
    "        n, bins = np.histogram(EnsembleCentrality[character])\n",
    "\n",
    "        NumberOfBins = 1000\n",
    "\n",
    "        binsc = .5*( bins[1:] + bins[:-1] )\n",
    "        bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "        bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "        n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "        diff = [np.abs(x-actualc) for x in bins2c]\n",
    "        ind = diff.index(np.min(diff))\n",
    "\n",
    "        Pval[character] = np.sum(n2[ind:])/np.sum(n2)\n",
    "    \n",
    "    return Pval\n",
    "    \n",
    "PvalB = NullModelPvalues(CharacterBetweenness, EnsembleBC)\n",
    "PvalC = NullModelPvalues(CharacterCloseness, EnsembleCC)\n",
    "PvalE = NullModelPvalues(CharacterCloseness, EnsembleEC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pval = PvalE\n",
    "RawCentrality = CharacterEig\n",
    "\n",
    "ShowNames = sorted(CharacterNodes, key=lambda x: RawCentrality[x], reverse=True)[:5]\n",
    "    \n",
    "RawRanking = {}\n",
    "AdjustedRanking = {}\n",
    "\n",
    "for index, character in enumerate(sorted(CharacterNodes, key=lambda x: RawCentrality[x], reverse=True)):\n",
    "    RawRanking[character] = index + 1\n",
    "    \n",
    "for index, character in enumerate(sorted(CharacterNodes, key=lambda x: Pval[x])):\n",
    "    AdjustedRanking[character] = index + 1\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    plt.plot(RawRanking[character],AdjustedRanking[character],'.')\n",
    "    if character in ShowNames:\n",
    "        plt.annotate(character,(RawRanking[character],AdjustedRanking[character]),color='k',fontsize = 10)\n",
    "plt.xlabel('Raw centrality ranking', fontsize=14)\n",
    "plt.ylabel('Adjusted centrality ranking', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('Adjusted ranking, Raw ranking, Node, Degree, Raw centrality value, P-value')\n",
    "print('__________________________________________________________________')\n",
    "for character in sorted(CharacterNodes, key=lambda x: Pval[x]):\n",
    "    print(AdjustedRanking[character],RawRanking[character],character,CharacterDegree[character],round(RawCentrality[character],4),round(Pval[character],4))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality vs. degree: Empirical and null model values\n",
    "\n",
    "The code below will plot empirical centrality values as a function of degree atop the corresponding null model values. Adjust the list variable \"ShowNames\" to choose which characters will be labelled in these plots.\n",
    "\n",
    "The cyan-colored points are values for null model network realizations, blue points are the null model mean values, and red points are the empirical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShowNames = sorted(CharacterNodes, key=lambda x: CharacterBetweenness[x], reverse=True)[:5]\n",
    "\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    plt.plot(CharacterDegree[character]*np.ones(len(EnsembleDegree[character])),EnsembleDegree[character],'c.',markersize=1)\n",
    "    plt.plot(CharacterDegree[character],np.mean(EnsembleDegree[character]),'b.')\n",
    "    if character in ShowNames:\n",
    "        plt.annotate(character,(CharacterDegree[character],CharacterDegree[character]),color='k',fontsize = 10)\n",
    "plt.plot(cdeg,cdeg,'r.')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Degree (null model)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    plt.plot(CharacterDegree[character]*np.ones(len(EnsembleBC[character])),EnsembleBC[character],'c.',markersize=1)\n",
    "    plt.plot(CharacterDegree[character],np.mean(EnsembleBC[character]),'b.')\n",
    "    if character in ShowNames:\n",
    "        plt.annotate(character,(CharacterDegree[character],CharacterBetweenness[character]),color='k',fontsize = 10)\n",
    "plt.plot(cdeg,cbet,'r.')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Betweenness centrality', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    plt.plot(CharacterDegree[character]*np.ones(len(EnsembleCC[character])),EnsembleCC[character],'c.',markersize=1)\n",
    "    plt.plot(CharacterDegree[character],np.mean(EnsembleCC[character]),'b.')    \n",
    "    if character in ShowNames:\n",
    "        plt.annotate(character,(CharacterDegree[character],CharacterCloseness[character]),color='k',fontsize = 10)\n",
    "plt.plot(cdeg,cclo,'r.')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Closeness centrality', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "for character in CharacterNodes:\n",
    "    plt.plot(CharacterDegree[character]*np.ones(len(EnsembleEC[character])),EnsembleEC[character],'c.',markersize=1)\n",
    "    plt.plot(CharacterDegree[character],np.mean(EnsembleEC[character]),'b.')    \n",
    "    if character in ShowNames:\n",
    "        plt.annotate(character,(CharacterDegree[character],CharacterEig[character]),color='k',fontsize = 10)\n",
    "plt.plot(cdeg,ceig,'r.')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Eigenvector centrality', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "* What (if anything) did you learn from the plot of $p$-value rankings versus \"raw\" centrality rankings? Can the null model tell us anything that the centrality measures alone don't?\n",
    "\n",
    "* Did you observe any characters that have low degree, and low raw centrality values, but greatly exceed the null model expectation (that is, had a low $p$-value)? What does this indicate?\n",
    "\n",
    "* Do characters that share the above description have anything in common?\n",
    "\n",
    "* What about characters which have high degree and high centrality, and yet also exceed the null model expectation? Does this tell us anything we didn't know before? How does this distinguish them from other high-degree characters whose centrality values conform more closely to the null model expectation?\n",
    "\n",
    "* What would a very high $p$-value (approaching 1) indicate? Do you observe any outliers with very high $p$-value? Do these characters share anything in common?\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Going further\n",
    "\n",
    "As a next step, here are some challenges to try. Some relevant codes are attached below, but feel free to try your own approach too.\n",
    "\n",
    "* __Null model analysis of \"global\" network metrics:__ The null model analyses generated above kept track of the following \"global\" network measures in dictionaries: 'EnsembleDiameter', 'EnsembleShortestPath', 'EnsembleDensity', 'EnsembleClustering'. Compare the null model values to the empirical co-occurrence network's properties (Example code is included below). Which metrics differ significantly from their null model expectations? Do these deviations make sense? How do these deviations compare with what we would expect from real-world social networks? Are there any other interesting global network metrics available that would be worth checking? If so, try it.\n",
    "\n",
    "* __Visualizing network outliers:__ Export null model information and use Gephi to create network visualizations that compare raw centrality measures to their null model $p$-values (you may find the code below to be of use). Does the shift of perspective from raw centrality measures to *p*-values reveal any interesting, previously-hidden structural features?\n",
    "\n",
    "* __Rewiring model:__ Instead of using a configuration model to generate null model networks, try creating a null model ensemble by randomly rewiring links while preserving the empirical network's degree sequence (you may find the codes included below to be useful). What advantages does this offer over a configuration model? What additional challenges does it present?\n",
    "\n",
    "* __Episode intersection network:__ Here we have projected the bipartite network of character-episode affiliations onto the set of character nodes to yield weighted _character co-occurrence networks_. If we instead project onto the set of episode nodes, we get am _episode intersection network_ where nodes represent episodes and link weights represent the number of characters appearing in both of the linked episodes. Repeat some of the above analyses on this network (which was actually already generated above as a networkx graph \"E\" when we imported our networks). How do you interpret centrality measures in this network? What kinds of new questions might this network allow us to investigate?\n",
    "\n",
    "* __Link betweenness centrality:__ Apply null model analysis to _edge betweenness centrality_ in order to identify _edges_ ( _links_ ) with higher-than-expected betweenness centrality values. How might we interpret these outlier links? Export your results and visualize the results in Gephi along with node-based betweenness centrality values. Does this reveal any interesting new structural features of the network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample codes\n",
    "\n",
    "#### Sample codes for \"rewiring model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RewireBipartite(B, EpisodeDegrees, CharacterDegrees, NumberOfIterations):\n",
    "\n",
    "    for t in range(NumberOfIterations):\n",
    "        char1 = np.random.choice(CharacterNodes)\n",
    "        episode1 = np.random.choice(list(B.neighbors(char1)))\n",
    "        char2 = np.random.choice(CharacterNodes)\n",
    "        episode2 = np.random.choice(list(B.neighbors(char2)))\n",
    "        while (char1==char2) or (episode1==episode2) or \\\n",
    "            B.has_edge(char1,episode2) or B.has_edge(char2,episode1):\n",
    "            char1 = np.random.choice(CharacterNodes)\n",
    "            episode1 = np.random.choice(list(B.neighbors(char1)))\n",
    "            char2 = np.random.choice(CharacterNodes)\n",
    "            episode2 = np.random.choice(list(B.neighbors(char2)))\n",
    "        B.remove_edge(char1,episode1)\n",
    "        B.remove_edge(char2,episode2)\n",
    "        B.add_edge(char1,episode2)\n",
    "        B.add_edge(char2,episode1)\n",
    "        if (nx.number_connected_components(B)>1):\n",
    "            B.add_edge(char1,episode1)\n",
    "            B.add_edge(char2,episode2)\n",
    "            B.remove_edge(char1,episode2)\n",
    "            B.remove_edge(char2,episode1)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleSize = 30\n",
    "NumberOfRewirings = 10\n",
    "\n",
    "EpisodeDegree = dict(B.degree(EpisodeNodes))\n",
    "\n",
    "EnsembleBC = {}\n",
    "RunningMeanBC = {}\n",
    "for character in CharacterNodes:\n",
    "    EnsembleBC[character] = []\n",
    "    RunningMeanBC[character] = []\n",
    "\n",
    "for t in range(EnsembleSize):\n",
    "    C = RewireBipartite(B, EpisodeDegree, CharacterDegree, NumberOfRewirings)\n",
    "    G = CharacterCooccurrenceNetwork(C, EpisodeNodes, CharacterNodes)\n",
    "    GI = InverseWeightNetwork(G)\n",
    "    CBetweenness = nx.betweenness_centrality(GI, weight='weight')\n",
    "    for character in CharacterNodes:\n",
    "        EnsembleBC[character].append(CBetweenness[character])\n",
    "        RunningMeanBC[character].append(np.mean(EnsembleBC[character]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code for null model analysis of \"global\" network metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = nx.average_clustering(G)\n",
    "EnsembleM = EnsembleClustering\n",
    "\n",
    "n, bins = np.histogram(EnsembleM)\n",
    "\n",
    "NumberOfBins = 1000\n",
    "\n",
    "binsc = .5*( bins[1:] + bins[:-1] )\n",
    "bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "diff = [np.abs(x-actual) for x in bins2c]\n",
    "ind = diff.index(np.min(diff))\n",
    "\n",
    "Pval = np.sum(n2[ind:])/np.sum(n2)\n",
    "\n",
    "n, bins, __ = plt.hist(EnsembleM)\n",
    "\n",
    "NumberOfBins = 1000\n",
    "\n",
    "binsc = .5*( bins[1:] + bins[:-1] )\n",
    "bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "diff = [np.abs(x-actual) for x in bins2c]\n",
    "ind = diff.index(np.min(diff))\n",
    "pval = np.sum(n2[ind:])/np.sum(n2)\n",
    "\n",
    "plt.plot([actual,actual],[0,np.max(n)],'r--')\n",
    "plt.plot(bins2c,n2,'k')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.xlabel('Network density')\n",
    "plt.ylabel('Null model abundance')\n",
    "plt.show()\n",
    "\n",
    "CPD = np.cumsum(n2)/np.sum(n2)\n",
    "plt.plot(bins2c, CPD,'k')\n",
    "plt.plot([actualbc,actualbc],[0,CPD[ind]],'r--')\n",
    "plt.plot([0,bins2c[ind]],[CPD[ind],CPD[ind]],'r--')\n",
    "plt.xlim((bins2c[0],bins2c[-1]))\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel('Network density')\n",
    "plt.ylabel('Cumulative probability distribution')\n",
    "plt.title(\"$p$-value: 1 - \" + str(round(1-pval,4)) + \" = \" + str(round(pval,4)), fontsize=16 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample codes for visualization of centrality outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CooccurrenceNetworkNodesForGephi(dataset, CharacterNodes, Deg, Str, Bet, Clo, Eig, BetP, CloP, EigP ):\n",
    "\n",
    "    b = open(\"./gephi/\" + dataset + \"_CharactersNodeList.csv\" , 'w')\n",
    "    b.write(\"Id,Label,Degree,\" +\n",
    "        \"Node strength,Betweenness centrality,Closeness centrality,\" +\n",
    "        \"Eigenvector centrality,P-value (B.C.),P-value (B.C.),P-value (E.C.)\\n\")\n",
    "    for char in CharacterNodes:\n",
    "        charp = char.replace(\",\",\"\")\n",
    "        b.write(charp + \",\" + charp + \",\" + \n",
    "            str(Deg[char]) + \",\" + \n",
    "            str(Str[char]) + \",\" + \n",
    "            str(Bet[char]) + \",\" +\n",
    "            str(Clo[char]) + \",\" +\n",
    "            str(Eig[char]) + \",\" + \n",
    "            str(BetP[char]) + \",\" +\n",
    "            str(CloP[char]) + \",\" +\n",
    "            str(EigP[char]) + \"\\n\" )\n",
    "        \n",
    "    b.close()\n",
    "\n",
    "def CooccurrenceNetworkEdgesForGephi(datasettag, G, LinkBetweenness=[], PvalLB=[]):\n",
    "\n",
    "    b = open(\"./gephi/\" + datasettag + \"_CharactersEdgeList.csv\" , 'w')\n",
    "    b.write(\"Source,Target,Weight\")\n",
    "    if LinkBetweenness:\n",
    "        b.write(\",Link betweenness centrality\")\n",
    "    if PvalLB:\n",
    "        b.write(\",P-value (Link B.C.)\")\n",
    "    b.write(\"\\n\")\n",
    "\n",
    "    for edge in sorted(list(G.edges())):\n",
    "        edge0 = edge[0].replace(\",\",\"\")\n",
    "        edge1 = edge[1].replace(\",\",\"\")\n",
    "        b.write(edge0 + \",\" + edge1 + \",\" + str(G[edge[0]][edge[1]]['weight']) )\n",
    "        if LinkBetweenness:\n",
    "            b.write(\",\" + str(LinkBetweenness[edge]))\n",
    "        if PvalLB:\n",
    "            b.write(\",\" + str(PvalLB[edge]))\n",
    "        b.write(\"\\n\")\n",
    "        \n",
    "    b.close()\n",
    "\n",
    "CooccurrenceNetworkNodesForGephi(dataset, CharacterNodes, CharacterDegree,\n",
    "    CharacterNodeStrength, CharacterBetweenness, CharacterCloseness, CharacterEig,\n",
    "    PvalB, PvalC, PvalE )\n",
    "\n",
    "CooccurrenceNetworkEdgesForGephi(dataset, G) #, LinkBetweenness, PvalLBC)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code for link betweenness centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NullModelPvaluesLinks(RawCentrality,EnsembleCentrality):\n",
    "\n",
    "    Pval = {}\n",
    "\n",
    "    for edge in RawCentrality.keys():\n",
    "\n",
    "        actual = RawCentrality[edge]\n",
    "\n",
    "        n, bins = np.histogram(EnsembleCentrality[edge])\n",
    "\n",
    "        NumberOfBins = 1000\n",
    "\n",
    "        binsc = .5*( bins[1:] + bins[:-1] )\n",
    "        bins2 = np.linspace(binsc[0],binsc[-1],NumberOfBins)\n",
    "        bins2c = .5*( bins2[1:] + bins2[:-1] )\n",
    "        n2 = np.interp(bins2c,binsc,n)\n",
    "\n",
    "        diff = [np.abs(x-actual) for x in bins2c]\n",
    "        ind = diff.index(np.min(diff))\n",
    "\n",
    "        Pval[edge] = np.sum(n2[ind:])/np.sum(n2)\n",
    "    \n",
    "    return Pval\n",
    "\n",
    "LinkBetweenness = nx.edge_betweenness_centrality( GI , weight=\"weight\", normalized=True )\n",
    "PvalLBC = NullModelPvaluesLinks(LinkBetweenness,EnsembleLinkBC)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
